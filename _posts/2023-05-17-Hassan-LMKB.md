---
layout: post
title: "Hassan presents Language Models as Knowledge Bases: On Entity Representations, Storage Capacity, and Paraphrased Queries"
root: ../../
---

At this week's lab meeting, Hassan will present [the following work](https://aclanthology.org/2021.eacl-main.153.pdf) which appeared at ACL 2021: 

**Abstract:** Pretrained language models have been suggested as a possible alternative or complement to structured knowledge bases. However, this emerging LM-as-KB paradigm has so far only been considered in a very limited setting, which only allows handling 21k entities whose name is found in common LM vocabularies. Furthermore, a major benefit of this paradigm, i.e., querying the KB using natural language paraphrases, is underexplored. Here we formulate two basic requirements for treating LMs as KBs: (i) the ability to store a large number facts involving a large number of entities and (ii) the ability to query stored facts. We explore three entity representations that allow LMs to handle millions of entities and present a detailed case study on paraphrased querying of facts stored in LMs, thereby providing a proof-of-concept that language models can indeed serve as knowledge bases.

[Wednesday, 17 May at 12pm](https://calndr.link/event/tBQcmcZlpx) -- <ins>click to add to calendar</ins>.

This will be a hybrid meeting at ASB 9921. The zoom link will be posted on zulip.
