---
layout: post
title: "Pooya and Zhenqi will present their recent papers due to appear in EMNLP 2019."
root: ../../
---
In our lab meeting tomorrow Zhenqi and Pooya will present their EMNLP2019 papers. Here is the title and abstract of their talks:

**Interrogating the Explanatory Power of Attention in Neural Machine Translation**

**Abstract:** *Attention models have become a crucial component in neural machine translation (NMT). They are often implicitly or explicitly used to justify the modelâ€™s decision in generating a specific token but it has not yet been rigorously established to what extent attention is a reliable source of information in NMT. To evaluate the explanatory power of attention for NMT, we examine the possibility of yielding the same prediction but with counterfactual attention models that modify crucial aspects of the trained attention model. Using these counterfactual attention mechanisms we assess the extent to which they still preserve the generation of function and content words in the translation process. Compared to a state of the art attention model, our counterfactual attention models produce 68% of function words and 21% of content words in our German-English dataset. Our experiments demonstrate that attention models by themselves cannot reliably explain the decisions made by a NMT model.*

**Deconstructing Supertagging into Multi-task Sequence Prediction**

**Abstract:** *In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularisation effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7{%} (2.2{%} absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.*

Tuesday, October 22th, 10:30 a.m. TASC1 9408.
